{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgPTEgysLut7"
   },
   "source": [
    "### Лабораторная работа №6: \"Классификация текста\"\n",
    "#### ИУ5-21 Курганова Александра\n",
    "#### Задание:\n",
    "Для произвольного набора данных, предназначенного для классификации текстов решите задачу классификации текста двумя способами:\n",
    "* Способ 1. На основе CountVectorizer или TfidfVectorizer.\n",
    "* Способ 2. На основе моделей word2vec или Glove или fastText.\n",
    "* Сравните качество полученных моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JITWe0REMS0v"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Tuple\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error, median_absolute_error, r2_score \n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.svm import SVC, NuSVC, LinearSVC, OneClassSVM, SVR, NuSVR, LinearSVR\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "\n",
    "%matplotlib inline \n",
    "sns.set(style=\"ticks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GGLV75yAMomw"
   },
   "outputs": [],
   "source": [
    "categories = [\"talk.politics.guns\", \"alt.atheism\", \"sci.med\", \"rec.autos\"]\n",
    "newsgroups = fetch_20newsgroups(subset='train', categories=categories)\n",
    "data = newsgroups['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EA6VuYfBM5fQ"
   },
   "outputs": [],
   "source": [
    "def accuracy_score_for_classes(\n",
    "    y_true: np.ndarray, \n",
    "    y_pred: np.ndarray) -> Dict[int, float]:\n",
    "    \"\"\"\n",
    "    Вычисление метрики accuracy для каждого класса\n",
    "    y_true - истинные значения классов\n",
    "    y_pred - предсказанные значения классов\n",
    "    Возвращает словарь: ключ - метка класса, \n",
    "    значение - Accuracy для данного класса\n",
    "    \"\"\"\n",
    "    # Для удобства фильтрации сформируем Pandas DataFrame \n",
    "    d = {'t': y_true, 'p': y_pred}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    # Метки классов\n",
    "    classes = np.unique(y_true)\n",
    "    # Результирующий словарь\n",
    "    res = dict()\n",
    "    # Перебор меток классов\n",
    "    for c in classes:\n",
    "        # отфильтруем данные, которые соответствуют \n",
    "        # текущей метке класса в истинных значениях\n",
    "        temp_data_flt = df[df['t']==c]\n",
    "        # расчет accuracy для заданной метки класса\n",
    "        temp_acc = accuracy_score(\n",
    "            temp_data_flt['t'].values, \n",
    "            temp_data_flt['p'].values)\n",
    "        # сохранение результата в словарь\n",
    "        res[c] = temp_acc\n",
    "    return res\n",
    "\n",
    "def print_accuracy_score_for_classes(\n",
    "    y_true: np.ndarray, \n",
    "    y_pred: np.ndarray):\n",
    "    \"\"\"\n",
    "    Вывод метрики accuracy для каждого класса\n",
    "    \"\"\"\n",
    "    accs = accuracy_score_for_classes(y_true, y_pred)\n",
    "    if len(accs)>0:\n",
    "        print('Метка \\t Accuracy')\n",
    "    for i in accs:\n",
    "        print('{} \\t {}'.format(i, accs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0KbG6oraNGYC",
    "outputId": "850d4eb1-a025-4b8e-f413-d10df8db29a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество сформированных признаков - 37176\n"
     ]
    }
   ],
   "source": [
    "# CountVectorizer\n",
    "vocabVect = CountVectorizer()\n",
    "vocabVect.fit(data)\n",
    "corpusVocab = vocabVect.vocabulary_\n",
    "print('Количество сформированных признаков - {}'.format(len(corpusVocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wcsRwP4oNJiP",
    "outputId": "1a5b2cbd-61b3-4d91-fb1f-e079d070288f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thom=33375\n",
      "morgan=23251\n",
      "ucs=34360\n",
      "mun=23527\n",
      "ca=8754\n",
      "thomas=33376\n",
      "clancy=9784\n",
      "subject=32210\n",
      "re=28101\n"
     ]
    }
   ],
   "source": [
    "for i in list(corpusVocab)[1:10]:\n",
    "    print('{}={}'.format(i, corpusVocab[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hKcujyqYO-JF",
    "outputId": "66b1e79f-e5f4-4609-f5e3-510f8f9eaf51"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2214x37176 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 375168 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features = vocabVect.transform(data)\n",
    "test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "as6wBtSTPcf8",
    "outputId": "29f6d3fb-995b-4bf2-8d93-6faf0bbee763"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37176"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Размер нулевой строки\n",
    "len(test_features.todense()[0].getA1())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UgN5li-lPkH_",
    "outputId": "418b8260-e5dd-4a14-a737-f3523095145f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 6,\n",
       " 11,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 10,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 10,\n",
       " 3]"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Непустые значения нулевой строки\n",
    "[i for i in test_features.todense()[0].getA1() if i>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VTwJUTxnPne2",
    "outputId": "58a48681-301a-46c3-df9f-7d00b2fb9eee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['025818u28037',\n",
       " '025924',\n",
       " '0278',\n",
       " '02908',\n",
       " '03',\n",
       " '030031',\n",
       " '030105',\n",
       " '030334',\n",
       " '0306',\n",
       " '030706',\n",
       " '030734',\n",
       " '031423',\n",
       " '0318',\n",
       " '0320',\n",
       " '032251',\n",
       " '032620',\n",
       " '032905',\n",
       " '033',\n",
       " '033446',\n",
       " '034']"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabVect.get_feature_names()[100:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AkqZqS-eP0mO"
   },
   "outputs": [],
   "source": [
    "def VectorizeAndClassify(vectorizers_list, classifiers_list):\n",
    "    for v in vectorizers_list:\n",
    "        for c in classifiers_list:\n",
    "            pipeline1 = Pipeline([(\"vectorizer\", v), (\"classifier\", c)])\n",
    "            score = cross_val_score(pipeline1, newsgroups['data'], newsgroups['target'], scoring='accuracy', cv=3).mean()\n",
    "            print('Векторизация - {}'.format(v))\n",
    "            print('Модель для классификации - {}'.format(c))\n",
    "            print('Accuracy = {}'.format(score))\n",
    "            print('===========================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wYryW_soP6IZ",
    "outputId": "b291230a-aa9b-4214-9f57-72e9ce604520"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Векторизация - CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None,\n",
      "                vocabulary={'00': 0, '000': 1, '0000': 2, '0000001200': 3,\n",
      "                            '00014': 4, '000152': 5, '000406': 6,\n",
      "                            '0005111312': 7, '0005111312na3em': 8, '000601': 9,\n",
      "                            '000710': 10, '000mi': 11, '000miles': 12,\n",
      "                            '000s': 13, '001': 14, '0010': 15, '001004': 16,\n",
      "                            '001125': 17, '001319': 18, '001642': 19, '002': 20,\n",
      "                            '002142': 21, '002651': 22, '003': 23,\n",
      "                            '003258u19250': 24, '0033': 25, '003522': 26,\n",
      "                            '004': 27, '004021809': 28, '004158': 29, ...})\n",
      "Модель для классификации - LogisticRegression(C=3.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "Accuracy = 0.951219512195122\n",
      "===========================\n",
      "Векторизация - CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None,\n",
      "                vocabulary={'00': 0, '000': 1, '0000': 2, '0000001200': 3,\n",
      "                            '00014': 4, '000152': 5, '000406': 6,\n",
      "                            '0005111312': 7, '0005111312na3em': 8, '000601': 9,\n",
      "                            '000710': 10, '000mi': 11, '000miles': 12,\n",
      "                            '000s': 13, '001': 14, '0010': 15, '001004': 16,\n",
      "                            '001125': 17, '001319': 18, '001642': 19, '002': 20,\n",
      "                            '002142': 21, '002651': 22, '003': 23,\n",
      "                            '003258u19250': 24, '0033': 25, '003522': 26,\n",
      "                            '004': 27, '004021809': 28, '004158': 29, ...})\n",
      "Модель для классификации - LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "          verbose=0)\n",
      "Accuracy = 0.9543812104787714\n",
      "===========================\n",
      "Векторизация - CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None,\n",
      "                vocabulary={'00': 0, '000': 1, '0000': 2, '0000001200': 3,\n",
      "                            '00014': 4, '000152': 5, '000406': 6,\n",
      "                            '0005111312': 7, '0005111312na3em': 8, '000601': 9,\n",
      "                            '000710': 10, '000mi': 11, '000miles': 12,\n",
      "                            '000s': 13, '001': 14, '0010': 15, '001004': 16,\n",
      "                            '001125': 17, '001319': 18, '001642': 19, '002': 20,\n",
      "                            '002142': 21, '002651': 22, '003': 23,\n",
      "                            '003258u19250': 24, '0033': 25, '003522': 26,\n",
      "                            '004': 27, '004021809': 28, '004158': 29, ...})\n",
      "Модель для классификации - KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\n",
      "Accuracy = 0.6820234869015357\n",
      "===========================\n",
      "Векторизация - TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use...\n",
      "                vocabulary={'00': 0, '000': 1, '0000': 2, '0000001200': 3,\n",
      "                            '00014': 4, '000152': 5, '000406': 6,\n",
      "                            '0005111312': 7, '0005111312na3em': 8, '000601': 9,\n",
      "                            '000710': 10, '000mi': 11, '000miles': 12,\n",
      "                            '000s': 13, '001': 14, '0010': 15, '001004': 16,\n",
      "                            '001125': 17, '001319': 18, '001642': 19, '002': 20,\n",
      "                            '002142': 21, '002651': 22, '003': 23,\n",
      "                            '003258u19250': 24, '0033': 25, '003522': 26,\n",
      "                            '004': 27, '004021809': 28, '004158': 29, ...})\n",
      "Модель для классификации - LogisticRegression(C=3.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "Accuracy = 0.9769647696476964\n",
      "===========================\n",
      "Векторизация - TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use...\n",
      "                vocabulary={'00': 0, '000': 1, '0000': 2, '0000001200': 3,\n",
      "                            '00014': 4, '000152': 5, '000406': 6,\n",
      "                            '0005111312': 7, '0005111312na3em': 8, '000601': 9,\n",
      "                            '000710': 10, '000mi': 11, '000miles': 12,\n",
      "                            '000s': 13, '001': 14, '0010': 15, '001004': 16,\n",
      "                            '001125': 17, '001319': 18, '001642': 19, '002': 20,\n",
      "                            '002142': 21, '002651': 22, '003': 23,\n",
      "                            '003258u19250': 24, '0033': 25, '003522': 26,\n",
      "                            '004': 27, '004021809': 28, '004158': 29, ...})\n",
      "Модель для классификации - LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "          verbose=0)\n",
      "Accuracy = 0.9810298102981031\n",
      "===========================\n",
      "Векторизация - TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use...\n",
      "                vocabulary={'00': 0, '000': 1, '0000': 2, '0000001200': 3,\n",
      "                            '00014': 4, '000152': 5, '000406': 6,\n",
      "                            '0005111312': 7, '0005111312na3em': 8, '000601': 9,\n",
      "                            '000710': 10, '000mi': 11, '000miles': 12,\n",
      "                            '000s': 13, '001': 14, '0010': 15, '001004': 16,\n",
      "                            '001125': 17, '001319': 18, '001642': 19, '002': 20,\n",
      "                            '002142': 21, '002651': 22, '003': 23,\n",
      "                            '003258u19250': 24, '0033': 25, '003522': 26,\n",
      "                            '004': 27, '004021809': 28, '004158': 29, ...})\n",
      "Модель для классификации - KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\n",
      "Accuracy = 0.9037940379403794\n",
      "===========================\n"
     ]
    }
   ],
   "source": [
    "vectorizers_list = [CountVectorizer(vocabulary = corpusVocab), TfidfVectorizer(vocabulary = corpusVocab)]\n",
    "classifiers_list = [LogisticRegression(C=3.0), LinearSVC(), KNeighborsClassifier()]\n",
    "VectorizeAndClassify(vectorizers_list, classifiers_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hWI7Rp1JQHf0"
   },
   "outputs": [],
   "source": [
    "# word2vec \n",
    "# Подготовим корпус\n",
    "corpus = []\n",
    "stop_words = stopwords.words('english')\n",
    "tok = WordPunctTokenizer()\n",
    "for line in newsgroups['data']:\n",
    "    line1 = line.strip().lower()\n",
    "    line1 = re.sub(\"[^a-zA-Z]\",\" \", line1)\n",
    "    text_tok = tok.tokenize(line1)\n",
    "    text_tok1 = [w for w in text_tok if not w in stop_words]\n",
    "    corpus.append(text_tok1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5rujZ84uQoZH",
    "outputId": "79910cf0-9aa6-4869-e44b-6a581f3f7daa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['thom',\n",
       "  'morgan',\n",
       "  'ucs',\n",
       "  'mun',\n",
       "  'ca',\n",
       "  'thomas',\n",
       "  'clancy',\n",
       "  'subject',\n",
       "  'thrush',\n",
       "  'good',\n",
       "  'grief',\n",
       "  'candida',\n",
       "  'albicans',\n",
       "  'organization',\n",
       "  'memorial',\n",
       "  'university',\n",
       "  'newfoundland',\n",
       "  'lines',\n",
       "  'dyer',\n",
       "  'spdcc',\n",
       "  'com',\n",
       "  'steve',\n",
       "  'dyer',\n",
       "  'writes',\n",
       "  'article',\n",
       "  'apr',\n",
       "  'ucsvax',\n",
       "  'sdsu',\n",
       "  'edu',\n",
       "  'mccurdy',\n",
       "  'ucsvax',\n",
       "  'sdsu',\n",
       "  'edu',\n",
       "  'mccurdy',\n",
       "  'writes',\n",
       "  'dyer',\n",
       "  'beyond',\n",
       "  'rude',\n",
       "  'drink',\n",
       "  'yeah',\n",
       "  'yeah',\n",
       "  'yeah',\n",
       "  'threaten',\n",
       "  'rip',\n",
       "  'lips',\n",
       "  'snort',\n",
       "  'always',\n",
       "  'people',\n",
       "  'blinded',\n",
       "  'knowledge',\n",
       "  'unopen',\n",
       "  'anything',\n",
       "  'already',\n",
       "  'established',\n",
       "  'given',\n",
       "  'medical',\n",
       "  'community',\n",
       "  'know',\n",
       "  'surprised',\n",
       "  'outlook',\n",
       "  'duh',\n",
       "  'nice',\n",
       "  'see',\n",
       "  'steve',\n",
       "  'still',\n",
       "  'high',\n",
       "  'almighty',\n",
       "  'intellectual',\n",
       "  'prowess',\n",
       "  'tact',\n",
       "  'record',\n",
       "  'several',\n",
       "  'outbreaks',\n",
       "  'thrush',\n",
       "  'several',\n",
       "  'past',\n",
       "  'years',\n",
       "  'indication',\n",
       "  'immunosuppression',\n",
       "  'nutritional',\n",
       "  'deficiencies',\n",
       "  'taken',\n",
       "  'antobiotics',\n",
       "  'listen',\n",
       "  'thrush',\n",
       "  'recognized',\n",
       "  'clinical',\n",
       "  'syndrome',\n",
       "  'definite',\n",
       "  'characteristics',\n",
       "  'thrush',\n",
       "  'thrush',\n",
       "  'see',\n",
       "  'lesions',\n",
       "  'culture',\n",
       "  'treat',\n",
       "  'generally',\n",
       "  'responds',\n",
       "  'well',\n",
       "  'otherwise',\n",
       "  'immunocompromised',\n",
       "  'noring',\n",
       "  'anal',\n",
       "  'retentive',\n",
       "  'idee',\n",
       "  'fixe',\n",
       "  'fungal',\n",
       "  'infection',\n",
       "  'sinuses',\n",
       "  'even',\n",
       "  'category',\n",
       "  'walking',\n",
       "  'neurasthenics',\n",
       "  'convinced',\n",
       "  'candida',\n",
       "  'reading',\n",
       "  'quack',\n",
       "  'book',\n",
       "  'yawn',\n",
       "  'dentist',\n",
       "  'sees',\n",
       "  'fair',\n",
       "  'amount',\n",
       "  'thrush',\n",
       "  'recommended',\n",
       "  'acidophilous',\n",
       "  'began',\n",
       "  'taking',\n",
       "  'acidophilous',\n",
       "  'daily',\n",
       "  'basis',\n",
       "  'outbreaks',\n",
       "  'ceased',\n",
       "  'quit',\n",
       "  'taking',\n",
       "  'acidophilous',\n",
       "  'outbreaks',\n",
       "  'periodically',\n",
       "  'resumed',\n",
       "  'resumed',\n",
       "  'taking',\n",
       "  'acidophilous',\n",
       "  'outbreaks',\n",
       "  'since',\n",
       "  'exactly',\n",
       "  'question',\n",
       "  'steve',\n",
       "  'point',\n",
       "  'person',\n",
       "  'one',\n",
       "  'steve',\n",
       "  'dyer',\n",
       "  'nice',\n",
       "  'see',\n",
       "  'things',\n",
       "  'never',\n",
       "  'change',\n",
       "  'steve',\n",
       "  'ignorant',\n",
       "  'one',\n",
       "  'group',\n",
       "  'alternative',\n",
       "  'another',\n",
       "  'one',\n",
       "  'positive',\n",
       "  'thing',\n",
       "  'came',\n",
       "  'longer',\n",
       "  'bothering',\n",
       "  'folks',\n",
       "  'alternative',\n",
       "  'shame',\n",
       "  'people',\n",
       "  'suffer',\n",
       "  'others',\n",
       "  'may',\n",
       "  'breath',\n",
       "  'freely',\n",
       "  'sorry',\n",
       "  'wasting',\n",
       "  'bandwidth',\n",
       "  'folks',\n",
       "  'forget',\n",
       "  'bow',\n",
       "  'every',\n",
       "  'second',\n",
       "  'day',\n",
       "  'offer',\n",
       "  'first',\n",
       "  'born',\n",
       "  'almight',\n",
       "  'omniscient',\n",
       "  'omnipotent',\n",
       "  'mr',\n",
       "  'steve'],\n",
       " ['kempmp',\n",
       "  'phoenix',\n",
       "  'oulu',\n",
       "  'fi',\n",
       "  'petri',\n",
       "  'pihko',\n",
       "  'subject',\n",
       "  'concerning',\n",
       "  'god',\n",
       "  'morality',\n",
       "  'long',\n",
       "  'organization',\n",
       "  'university',\n",
       "  'oulu',\n",
       "  'finland',\n",
       "  'x',\n",
       "  'newsreader',\n",
       "  'tin',\n",
       "  'version',\n",
       "  'pl',\n",
       "  'lines',\n",
       "  'kind',\n",
       "  'argument',\n",
       "  'cries',\n",
       "  'comment',\n",
       "  'jbrown',\n",
       "  'batman',\n",
       "  'bmd',\n",
       "  'trw',\n",
       "  'com',\n",
       "  'wrote',\n",
       "  'article',\n",
       "  'apr',\n",
       "  'leland',\n",
       "  'stanford',\n",
       "  'edu',\n",
       "  'galahad',\n",
       "  'leland',\n",
       "  'stanford',\n",
       "  'edu',\n",
       "  'scott',\n",
       "  'compton',\n",
       "  'writes',\n",
       "  'jim',\n",
       "  'originally',\n",
       "  'wrote',\n",
       "  'god',\n",
       "  'create',\n",
       "  'disease',\n",
       "  'responsible',\n",
       "  'maladies',\n",
       "  'newborns',\n",
       "  'god',\n",
       "  'create',\n",
       "  'life',\n",
       "  'according',\n",
       "  'protein',\n",
       "  'code',\n",
       "  'mutable',\n",
       "  'evolve',\n",
       "  'without',\n",
       "  'delving',\n",
       "  'deep',\n",
       "  'discussion',\n",
       "  'creationism',\n",
       "  'vs',\n",
       "  'evolutionism',\n",
       "  'god',\n",
       "  'created',\n",
       "  'original',\n",
       "  'genetic',\n",
       "  'code',\n",
       "  'perfect',\n",
       "  'without',\n",
       "  'flaw',\n",
       "  'evidence',\n",
       "  'code',\n",
       "  'perfect',\n",
       "  'degraded',\n",
       "  'ever',\n",
       "  'since',\n",
       "  'evidence',\n",
       "  'favour',\n",
       "  'statement',\n",
       "  'perhaps',\n",
       "  'biggest',\n",
       "  'imperfection',\n",
       "  'code',\n",
       "  'full',\n",
       "  'non',\n",
       "  'coding',\n",
       "  'regions',\n",
       "  'introns',\n",
       "  'called',\n",
       "  'intervene',\n",
       "  'coding',\n",
       "  'regions',\n",
       "  'exons',\n",
       "  'impressive',\n",
       "  'amount',\n",
       "  'evidence',\n",
       "  'suggests',\n",
       "  'introns',\n",
       "  'ancient',\n",
       "  'origin',\n",
       "  'likely',\n",
       "  'early',\n",
       "  'exons',\n",
       "  'represented',\n",
       "  'early',\n",
       "  'protein',\n",
       "  'domains',\n",
       "  'number',\n",
       "  'introns',\n",
       "  'decreasing',\n",
       "  'increasing',\n",
       "  'appears',\n",
       "  'intron',\n",
       "  'loss',\n",
       "  'occur',\n",
       "  'species',\n",
       "  'common',\n",
       "  'ancestry',\n",
       "  'usually',\n",
       "  'quite',\n",
       "  'similar',\n",
       "  'exon',\n",
       "  'intron',\n",
       "  'structure',\n",
       "  'genes',\n",
       "  'hand',\n",
       "  'possibility',\n",
       "  'introns',\n",
       "  'inserted',\n",
       "  'later',\n",
       "  'presents',\n",
       "  'several',\n",
       "  'logical',\n",
       "  'difficulties',\n",
       "  'introns',\n",
       "  'removed',\n",
       "  'splicing',\n",
       "  'mechanism',\n",
       "  'would',\n",
       "  'present',\n",
       "  'unused',\n",
       "  'introns',\n",
       "  'inserted',\n",
       "  'moreover',\n",
       "  'intron',\n",
       "  'insertion',\n",
       "  'would',\n",
       "  'required',\n",
       "  'precise',\n",
       "  'targeting',\n",
       "  'random',\n",
       "  'insertion',\n",
       "  'would',\n",
       "  'tolerated',\n",
       "  'since',\n",
       "  'sequences',\n",
       "  'intron',\n",
       "  'removal',\n",
       "  'self',\n",
       "  'splicing',\n",
       "  'mrna',\n",
       "  'conserved',\n",
       "  'besides',\n",
       "  'transposition',\n",
       "  'sequence',\n",
       "  'usually',\n",
       "  'leaves',\n",
       "  'trace',\n",
       "  'long',\n",
       "  'terminal',\n",
       "  'repeats',\n",
       "  'target',\n",
       "  'site',\n",
       "  'duplications',\n",
       "  'found',\n",
       "  'near',\n",
       "  'intron',\n",
       "  'sequences',\n",
       "  'seriously',\n",
       "  'recommend',\n",
       "  'reading',\n",
       "  'textbooks',\n",
       "  'molecular',\n",
       "  'biology',\n",
       "  'genetics',\n",
       "  'posting',\n",
       "  'theological',\n",
       "  'arguments',\n",
       "  'like',\n",
       "  'try',\n",
       "  'watson',\n",
       "  'molecular',\n",
       "  'biology',\n",
       "  'gene',\n",
       "  'darnell',\n",
       "  'lodish',\n",
       "  'baltimore',\n",
       "  'molecular',\n",
       "  'biology',\n",
       "  'cell',\n",
       "  'starters',\n",
       "  'remember',\n",
       "  'question',\n",
       "  'posed',\n",
       "  'theological',\n",
       "  'context',\n",
       "  'god',\n",
       "  'cause',\n",
       "  'disease',\n",
       "  'newborns',\n",
       "  'answer',\n",
       "  'likewise',\n",
       "  'theological',\n",
       "  'perspective',\n",
       "  'less',\n",
       "  'valid',\n",
       "  'purely',\n",
       "  'scientific',\n",
       "  'perspective',\n",
       "  'different',\n",
       "  'scientific',\n",
       "  'perspective',\n",
       "  'supported',\n",
       "  'evidence',\n",
       "  'whereas',\n",
       "  'theological',\n",
       "  'perspectives',\n",
       "  'often',\n",
       "  'fail',\n",
       "  'fulfil',\n",
       "  'criterion',\n",
       "  'think',\n",
       "  'misread',\n",
       "  'meaning',\n",
       "  'said',\n",
       "  'god',\n",
       "  'made',\n",
       "  'genetic',\n",
       "  'code',\n",
       "  'perfect',\n",
       "  'mean',\n",
       "  'perfect',\n",
       "  'certainly',\n",
       "  'evolved',\n",
       "  'since',\n",
       "  'worse',\n",
       "  'would',\n",
       "  'please',\n",
       "  'cite',\n",
       "  'references',\n",
       "  'support',\n",
       "  'assertion',\n",
       "  'assertion',\n",
       "  'less',\n",
       "  'valid',\n",
       "  'scientific',\n",
       "  'perspective',\n",
       "  'unless',\n",
       "  'support',\n",
       "  'evidence',\n",
       "  'fact',\n",
       "  'claimed',\n",
       "  'parasites',\n",
       "  'diseases',\n",
       "  'perhaps',\n",
       "  'important',\n",
       "  'thought',\n",
       "  'instance',\n",
       "  'sex',\n",
       "  'might',\n",
       "  'evolved',\n",
       "  'defence',\n",
       "  'parasites',\n",
       "  'view',\n",
       "  'supported',\n",
       "  'computer',\n",
       "  'simulations',\n",
       "  'evolution',\n",
       "  'eg',\n",
       "  'tierra',\n",
       "  'perhaps',\n",
       "  'thought',\n",
       "  'higher',\n",
       "  'energy',\n",
       "  'rays',\n",
       "  'like',\n",
       "  'x',\n",
       "  'rays',\n",
       "  'gamma',\n",
       "  'rays',\n",
       "  'cosmic',\n",
       "  'rays',\n",
       "  'caused',\n",
       "  'damage',\n",
       "  'fact',\n",
       "  'thermal',\n",
       "  'energy',\n",
       "  'damage',\n",
       "  'although',\n",
       "  'usually',\n",
       "  'mild',\n",
       "  'easily',\n",
       "  'fixed',\n",
       "  'enzymatic',\n",
       "  'action',\n",
       "  'actually',\n",
       "  'neither',\n",
       "  'us',\n",
       "  'knows',\n",
       "  'atmosphere',\n",
       "  'like',\n",
       "  'time',\n",
       "  'god',\n",
       "  'created',\n",
       "  'life',\n",
       "  'according',\n",
       "  'recollection',\n",
       "  'biologists',\n",
       "  'claim',\n",
       "  'life',\n",
       "  'began',\n",
       "  'billion',\n",
       "  'years',\n",
       "  'ago',\n",
       "  'would',\n",
       "  'half',\n",
       "  'billion',\n",
       "  'years',\n",
       "  'earth',\n",
       "  'created',\n",
       "  'would',\n",
       "  'still',\n",
       "  'primitive',\n",
       "  'support',\n",
       "  'life',\n",
       "  'seem',\n",
       "  'remember',\n",
       "  'figure',\n",
       "  'like',\n",
       "  'billion',\n",
       "  'years',\n",
       "  'ago',\n",
       "  'origination',\n",
       "  'life',\n",
       "  'earth',\n",
       "  'anyone',\n",
       "  'better',\n",
       "  'estimate',\n",
       "  'replace',\n",
       "  'created',\n",
       "  'formed',\n",
       "  'since',\n",
       "  'need',\n",
       "  'invoke',\n",
       "  'creator',\n",
       "  'earth',\n",
       "  'formed',\n",
       "  'without',\n",
       "  'one',\n",
       "  'recent',\n",
       "  'estimates',\n",
       "  'age',\n",
       "  'earth',\n",
       "  'range',\n",
       "  'billion',\n",
       "  'years',\n",
       "  'earliest',\n",
       "  'signs',\n",
       "  'life',\n",
       "  'true',\n",
       "  'fossils',\n",
       "  'organic',\n",
       "  'stromatolite',\n",
       "  'like',\n",
       "  'layers',\n",
       "  'date',\n",
       "  'back',\n",
       "  'billion',\n",
       "  'years',\n",
       "  'would',\n",
       "  'leave',\n",
       "  'billion',\n",
       "  'years',\n",
       "  'first',\n",
       "  'cells',\n",
       "  'evolve',\n",
       "  'sorry',\n",
       "  'give',\n",
       "  'references',\n",
       "  'based',\n",
       "  'course',\n",
       "  'evolutionary',\n",
       "  'biochemistry',\n",
       "  'attended',\n",
       "  'dominion',\n",
       "  'great',\n",
       "  'feat',\n",
       "  'satan',\n",
       "  'genetically',\n",
       "  'engineer',\n",
       "  'diseases',\n",
       "  'bacterial',\n",
       "  'viral',\n",
       "  'genetic',\n",
       "  'although',\n",
       "  'forces',\n",
       "  'natural',\n",
       "  'selection',\n",
       "  'tend',\n",
       "  'improve',\n",
       "  'survivability',\n",
       "  'species',\n",
       "  'degeneration',\n",
       "  'genetic',\n",
       "  'code',\n",
       "  'tends',\n",
       "  'offset',\n",
       "  'want',\n",
       "  'true',\n",
       "  'evidence',\n",
       "  'supposed',\n",
       "  'degeneration',\n",
       "  'understand',\n",
       "  'scott',\n",
       "  'reaction',\n",
       "  'excuse',\n",
       "  'far',\n",
       "  'fetched',\n",
       "  'know',\n",
       "  'must',\n",
       "  'jesting',\n",
       "  'know',\n",
       "  'pathogens',\n",
       "  'know',\n",
       "  'point',\n",
       "  'mutations',\n",
       "  'know',\n",
       "  'everything',\n",
       "  'come',\n",
       "  'spontaneously',\n",
       "  'response',\n",
       "  'last',\n",
       "  'statement',\n",
       "  'neither',\n",
       "  'may',\n",
       "  'well',\n",
       "  'believe',\n",
       "  'accept',\n",
       "  'fact',\n",
       "  'cannot',\n",
       "  'know',\n",
       "  'hope',\n",
       "  'forget',\n",
       "  'evidence',\n",
       "  'suggests',\n",
       "  'everything',\n",
       "  'come',\n",
       "  'spontaneously',\n",
       "  'evidence',\n",
       "  'conclusion',\n",
       "  'science',\n",
       "  'one',\n",
       "  'believe',\n",
       "  'anything',\n",
       "  'healthy',\n",
       "  'sign',\n",
       "  'doubt',\n",
       "  'disbelieve',\n",
       "  'right',\n",
       "  'path',\n",
       "  'walk',\n",
       "  'take',\n",
       "  'look',\n",
       "  'evidence',\n",
       "  'present',\n",
       "  'one',\n",
       "  'conclusions',\n",
       "  'prior',\n",
       "  'theology',\n",
       "  'use',\n",
       "  'method',\n",
       "  'therefore',\n",
       "  'seriously',\n",
       "  'doubt',\n",
       "  'could',\n",
       "  'ever',\n",
       "  'come',\n",
       "  'right',\n",
       "  'conclusions',\n",
       "  'human',\n",
       "  'dna',\n",
       "  'complex',\n",
       "  'tends',\n",
       "  'accumulate',\n",
       "  'errors',\n",
       "  'adversely',\n",
       "  'affecting',\n",
       "  'well',\n",
       "  'ability',\n",
       "  'fight',\n",
       "  'disease',\n",
       "  'simpler',\n",
       "  'dna',\n",
       "  'bacteria',\n",
       "  'viruses',\n",
       "  'tend',\n",
       "  'become',\n",
       "  'efficient',\n",
       "  'causing',\n",
       "  'infection',\n",
       "  'disease',\n",
       "  'bad',\n",
       "  'combination',\n",
       "  'hence',\n",
       "  'newborns',\n",
       "  'suffer',\n",
       "  'genetic',\n",
       "  'viral',\n",
       "  'bacterial',\n",
       "  'diseases',\n",
       "  'disorders',\n",
       "  'supposing',\n",
       "  'purpose',\n",
       "  'valid',\n",
       "  'move',\n",
       "  'bacteria',\n",
       "  'viruses',\n",
       "  'exist',\n",
       "  'cause',\n",
       "  'disease',\n",
       "  'another',\n",
       "  'manifests',\n",
       "  'general',\n",
       "  'principle',\n",
       "  'evolution',\n",
       "  'replication',\n",
       "  'saves',\n",
       "  'replicators',\n",
       "  'degradiation',\n",
       "  'efficient',\n",
       "  'method',\n",
       "  'dna',\n",
       "  'survive',\n",
       "  'replicate',\n",
       "  'less',\n",
       "  'efficient',\n",
       "  'methods',\n",
       "  'make',\n",
       "  'present',\n",
       "  'last',\n",
       "  'time',\n",
       "  'please',\n",
       "  'present',\n",
       "  'evidence',\n",
       "  'claim',\n",
       "  'human',\n",
       "  'dna',\n",
       "  'degrading',\n",
       "  'evolutionary',\n",
       "  'processes',\n",
       "  'people',\n",
       "  'claimed',\n",
       "  'opposite',\n",
       "  'true',\n",
       "  'suppressed',\n",
       "  'selection',\n",
       "  'thus',\n",
       "  'bound',\n",
       "  'degrade',\n",
       "  'seen',\n",
       "  'much',\n",
       "  'evidence',\n",
       "  'either',\n",
       "  'claim',\n",
       "  'ask',\n",
       "  'relevant',\n",
       "  'discussion',\n",
       "  'answering',\n",
       "  'john',\n",
       "  'question',\n",
       "  'genetic',\n",
       "  'diseases',\n",
       "  'many',\n",
       "  'bacterial',\n",
       "  'viral',\n",
       "  'diseases',\n",
       "  'require',\n",
       "  'babies',\n",
       "  'develop',\n",
       "  'antibodies',\n",
       "  'god',\n",
       "  'fault',\n",
       "  'original',\n",
       "  'question',\n",
       "  'say',\n",
       "  'course',\n",
       "  'nothing',\n",
       "  'evil',\n",
       "  'god',\n",
       "  'fault',\n",
       "  'explanation',\n",
       "  'work',\n",
       "  'fails',\n",
       "  'miserably',\n",
       "  'may',\n",
       "  'right',\n",
       "  'fact',\n",
       "  'know',\n",
       "  'satan',\n",
       "  'responsible',\n",
       "  'neither',\n",
       "  'suppose',\n",
       "  'powerful',\n",
       "  'evil',\n",
       "  'like',\n",
       "  'satan',\n",
       "  'exists',\n",
       "  'would',\n",
       "  'inconceivable',\n",
       "  'might',\n",
       "  'responsible',\n",
       "  'many',\n",
       "  'ills',\n",
       "  'affect',\n",
       "  'mankind',\n",
       "  'think',\n",
       "  'could',\n",
       "  'done',\n",
       "  'much',\n",
       "  'better',\n",
       "  'job',\n",
       "  'pun',\n",
       "  'intended',\n",
       "  'problem',\n",
       "  'seems',\n",
       "  'satan',\n",
       "  'necessary',\n",
       "  'explain',\n",
       "  'diseases',\n",
       "  'inevitable',\n",
       "  'product',\n",
       "  'evolution',\n",
       "  'say',\n",
       "  'seems',\n",
       "  'like',\n",
       "  'another',\n",
       "  'bad',\n",
       "  'inference',\n",
       "  'actually',\n",
       "  'done',\n",
       "  'oversimplify',\n",
       "  'said',\n",
       "  'point',\n",
       "  'summary',\n",
       "  'words',\n",
       "  'takes',\n",
       "  'new',\n",
       "  'context',\n",
       "  'never',\n",
       "  'said',\n",
       "  'people',\n",
       "  'meant',\n",
       "  'presumably',\n",
       "  'god',\n",
       "  'punished',\n",
       "  'getting',\n",
       "  'diseases',\n",
       "  'say',\n",
       "  'free',\n",
       "  'moral',\n",
       "  'choices',\n",
       "  'attendent',\n",
       "  'consequences',\n",
       "  'mankind',\n",
       "  'chooses',\n",
       "  'reject',\n",
       "  'god',\n",
       "  'people',\n",
       "  'done',\n",
       "  'since',\n",
       "  'beginning',\n",
       "  'expect',\n",
       "  'god',\n",
       "  'protect',\n",
       "  'adverse',\n",
       "  'events',\n",
       "  'entropic',\n",
       "  'universe',\n",
       "  'expecting',\n",
       "  'god',\n",
       "  'exists',\n",
       "  'expect',\n",
       "  'leave',\n",
       "  'us',\n",
       "  'alone',\n",
       "  'would',\n",
       "  'also',\n",
       "  'like',\n",
       "  'hear',\n",
       "  'believe',\n",
       "  'choices',\n",
       "  'indeed',\n",
       "  'free',\n",
       "  'interesting',\n",
       "  'philosophical',\n",
       "  'question',\n",
       "  'answer',\n",
       "  'clear',\n",
       "  'cut',\n",
       "  'seems',\n",
       "  'consequences',\n",
       "  'would',\n",
       "  'expect',\n",
       "  'rejecting',\n",
       "  'allah',\n",
       "  'oh',\n",
       "  'admit',\n",
       "  'perfect',\n",
       "  'yet',\n",
       "  'working',\n",
       "  'good',\n",
       "  'library',\n",
       "  'bookstore',\n",
       "  'good',\n",
       "  'starting',\n",
       "  'point',\n",
       "  'price',\n",
       "  'tea',\n",
       "  'china',\n",
       "  'question',\n",
       "  'provided',\n",
       "  'answer',\n",
       "  'biology',\n",
       "  'genetics',\n",
       "  'fine',\n",
       "  'subjects',\n",
       "  'important',\n",
       "  'scientific',\n",
       "  'endeavors',\n",
       "  'explain',\n",
       "  'god',\n",
       "  'created',\n",
       "  'set',\n",
       "  'life',\n",
       "  'processes',\n",
       "  'explain',\n",
       "  'behind',\n",
       "  'creation',\n",
       "  'life',\n",
       "  'subsequent',\n",
       "  'evolution',\n",
       "  'behind',\n",
       "  'proposition',\n",
       "  'something',\n",
       "  'supported',\n",
       "  'evidence',\n",
       "  'recommend',\n",
       "  'books',\n",
       "  'need',\n",
       "  'invoke',\n",
       "  'behind',\n",
       "  'prime',\n",
       "  'mover',\n",
       "  'evidence',\n",
       "  'whole',\n",
       "  'universe',\n",
       "  'come',\n",
       "  'existence',\n",
       "  'without',\n",
       "  'intervention',\n",
       "  'recent',\n",
       "  'cosmological',\n",
       "  'theories',\n",
       "  'hawking',\n",
       "  'et',\n",
       "  'al',\n",
       "  'suggest',\n",
       "  'people',\n",
       "  'still',\n",
       "  'insist',\n",
       "  'thanks',\n",
       "  'scotty',\n",
       "  'fine',\n",
       "  'sagely',\n",
       "  'advice',\n",
       "  'highly',\n",
       "  'motivated',\n",
       "  'learn',\n",
       "  'nitty',\n",
       "  'gritty',\n",
       "  'details',\n",
       "  'biology',\n",
       "  'genetics',\n",
       "  'although',\n",
       "  'sure',\n",
       "  'find',\n",
       "  'fascinating',\n",
       "  'subject',\n",
       "  'realize',\n",
       "  'details',\n",
       "  'change',\n",
       "  'big',\n",
       "  'picture',\n",
       "  'god',\n",
       "  'created',\n",
       "  'life',\n",
       "  'beginning',\n",
       "  'ability',\n",
       "  'change',\n",
       "  'adapt',\n",
       "  'environment',\n",
       "  'sorry',\n",
       "  'evidence',\n",
       "  'big',\n",
       "  'picture',\n",
       "  'need',\n",
       "  'create',\n",
       "  'anything',\n",
       "  'capable',\n",
       "  'adaptation',\n",
       "  'come',\n",
       "  'existence',\n",
       "  'without',\n",
       "  'supreme',\n",
       "  'try',\n",
       "  'reading',\n",
       "  'p',\n",
       "  'w',\n",
       "  'atkins',\n",
       "  'creation',\n",
       "  'revisited',\n",
       "  'freeman',\n",
       "  'petri',\n",
       "  'petri',\n",
       "  'pihko',\n",
       "  'kem',\n",
       "  'pmp',\n",
       "  'mathematics',\n",
       "  'truth',\n",
       "  'pihatie',\n",
       "  'c',\n",
       "  'finou',\n",
       "  'oulu',\n",
       "  'fi',\n",
       "  'physics',\n",
       "  'rule',\n",
       "  'sf',\n",
       "  'oulu',\n",
       "  'kempmp',\n",
       "  'game',\n",
       "  'finland',\n",
       "  'phoenix',\n",
       "  'oulu',\n",
       "  'fi',\n",
       "  'chemistry',\n",
       "  'game'],\n",
       " ['organization',\n",
       "  'university',\n",
       "  'illinois',\n",
       "  'chicago',\n",
       "  'academic',\n",
       "  'computer',\n",
       "  'center',\n",
       "  'jason',\n",
       "  'kratz',\n",
       "  'u',\n",
       "  'uicvm',\n",
       "  'uic',\n",
       "  'edu',\n",
       "  'subject',\n",
       "  'shoot',\n",
       "  'somebody',\n",
       "  'veal',\n",
       "  'utkvm',\n",
       "  'utk',\n",
       "  'edu',\n",
       "  'lines',\n",
       "  'article',\n",
       "  'veal',\n",
       "  'utkvm',\n",
       "  'utk',\n",
       "  'edu',\n",
       "  'veal',\n",
       "  'utkvm',\n",
       "  'utk',\n",
       "  'edu',\n",
       "  'david',\n",
       "  'veal',\n",
       "  'says',\n",
       "  'article',\n",
       "  'u',\n",
       "  'uicvm',\n",
       "  'uic',\n",
       "  'edu',\n",
       "  'jason',\n",
       "  'kratz',\n",
       "  'u',\n",
       "  'uicvm',\n",
       "  'uic',\n",
       "  'edu',\n",
       "  'heard',\n",
       "  'many',\n",
       "  'opinions',\n",
       "  'subject',\n",
       "  'would',\n",
       "  'like',\n",
       "  'hear',\n",
       "  'people',\n",
       "  'net',\n",
       "  'say',\n",
       "  'situation',\n",
       "  'pull',\n",
       "  'gun',\n",
       "  'somebody',\n",
       "  'give',\n",
       "  'chance',\n",
       "  'get',\n",
       "  'away',\n",
       "  'decided',\n",
       "  'continue',\n",
       "  'action',\n",
       "  'anyway',\n",
       "  'end',\n",
       "  'shooting',\n",
       "  'killing',\n",
       "  'question',\n",
       "  'stay',\n",
       "  'wait',\n",
       "  'cops',\n",
       "  'collect',\n",
       "  'brass',\n",
       "  'using',\n",
       "  'semi',\n",
       "  'auto',\n",
       "  'get',\n",
       "  'provided',\n",
       "  'course',\n",
       "  'think',\n",
       "  'seen',\n",
       "  'data',\n",
       "  'point',\n",
       "  'tennessee',\n",
       "  'friend',\n",
       "  'mine',\n",
       "  'police',\n",
       "  'officer',\n",
       "  'essentially',\n",
       "  'recommends',\n",
       "  'fade',\n",
       "  'away',\n",
       "  'even',\n",
       "  'perfectly',\n",
       "  'justified',\n",
       "  'likely',\n",
       "  'great',\n",
       "  'deal',\n",
       "  'hassle',\n",
       "  'side',\n",
       "  'note',\n",
       "  'carrying',\n",
       "  'gun',\n",
       "  'concealed',\n",
       "  'misdemeanor',\n",
       "  'exactly',\n",
       "  'heard',\n",
       "  'fade',\n",
       "  'away',\n",
       "  'nobody',\n",
       "  'saw',\n",
       "  'kind',\n",
       "  'evidence',\n",
       "  'would',\n",
       "  'able',\n",
       "  'get',\n",
       "  'catch',\n",
       "  'assuming',\n",
       "  'either',\n",
       "  'collected',\n",
       "  'brass',\n",
       "  'revolver',\n",
       "  'kind',\n",
       "  'laws',\n",
       "  'books',\n",
       "  'regarding',\n",
       "  'type',\n",
       "  'situation',\n",
       "  'would',\n",
       "  'likely',\n",
       "  'thing',\n",
       "  'happen',\n",
       "  'stayed',\n",
       "  'waited',\n",
       "  'first',\n",
       "  'offense',\n",
       "  'would',\n",
       "  'happen',\n",
       "  'took',\n",
       "  'someone',\n",
       "  'saw',\n",
       "  'caught',\n",
       "  'one',\n",
       "  'state',\n",
       "  'things',\n",
       "  'pretty',\n",
       "  'much',\n",
       "  'guess',\n",
       "  'time',\n",
       "  'take',\n",
       "  'trip',\n",
       "  'library',\n",
       "  'look',\n",
       "  'illinois',\n",
       "  'statutes',\n",
       "  'record',\n",
       "  'folks',\n",
       "  'asking',\n",
       "  'curious',\n",
       "  'trying',\n",
       "  'find',\n",
       "  'people',\n",
       "  'read',\n",
       "  'stuff',\n",
       "  'like',\n",
       "  'david',\n",
       "  'veal',\n",
       "  'univ',\n",
       "  'tenn',\n",
       "  'div',\n",
       "  'cont',\n",
       "  'education',\n",
       "  'info',\n",
       "  'services',\n",
       "  'group',\n",
       "  'pa',\n",
       "  'utkvm',\n",
       "  'utk',\n",
       "  'edu',\n",
       "  'still',\n",
       "  'remember',\n",
       "  'way',\n",
       "  'laughed',\n",
       "  'day',\n",
       "  'pushed',\n",
       "  'elevator',\n",
       "  'shaft',\n",
       "  'beginning',\n",
       "  'think',\n",
       "  'love',\n",
       "  'anymore',\n",
       "  'weird',\n",
       "  'al',\n",
       "  'jason',\n",
       "  'u',\n",
       "  'uicvm',\n",
       "  'cc',\n",
       "  'uic',\n",
       "  'edu'],\n",
       " ['geb',\n",
       "  'cs',\n",
       "  'pitt',\n",
       "  'edu',\n",
       "  'gordon',\n",
       "  'banks',\n",
       "  'subject',\n",
       "  'food',\n",
       "  'related',\n",
       "  'seizures',\n",
       "  'reply',\n",
       "  'geb',\n",
       "  'cs',\n",
       "  'pitt',\n",
       "  'edu',\n",
       "  'gordon',\n",
       "  'banks',\n",
       "  'organization',\n",
       "  'univ',\n",
       "  'pittsburgh',\n",
       "  'computer',\n",
       "  'science',\n",
       "  'lines',\n",
       "  'article',\n",
       "  'bu',\n",
       "  'edu',\n",
       "  'dozonoff',\n",
       "  'bu',\n",
       "  'edu',\n",
       "  'david',\n",
       "  'ozonoff',\n",
       "  'writes',\n",
       "  'many',\n",
       "  'cereals',\n",
       "  'corn',\n",
       "  'based',\n",
       "  'post',\n",
       "  'looked',\n",
       "  'literature',\n",
       "  'located',\n",
       "  'two',\n",
       "  'articles',\n",
       "  'implicated',\n",
       "  'corn',\n",
       "  'contains',\n",
       "  'tryptophan',\n",
       "  'seizures',\n",
       "  'idea',\n",
       "  'corn',\n",
       "  'diet',\n",
       "  'might',\n",
       "  'potentiate',\n",
       "  'already',\n",
       "  'existing',\n",
       "  'latent',\n",
       "  'seizure',\n",
       "  'disorder',\n",
       "  'cause',\n",
       "  'check',\n",
       "  'see',\n",
       "  'two',\n",
       "  'kellog',\n",
       "  'cereals',\n",
       "  'corn',\n",
       "  'based',\n",
       "  'interested',\n",
       "  'years',\n",
       "  'ago',\n",
       "  'intern',\n",
       "  'obese',\n",
       "  'young',\n",
       "  'woman',\n",
       "  'brought',\n",
       "  'er',\n",
       "  'comatose',\n",
       "  'reported',\n",
       "  'grand',\n",
       "  'mal',\n",
       "  'seizures',\n",
       "  'attending',\n",
       "  'corn',\n",
       "  'festival',\n",
       "  'pumped',\n",
       "  'stomach',\n",
       "  'obtained',\n",
       "  'seemed',\n",
       "  'like',\n",
       "  'couple',\n",
       "  'liters',\n",
       "  'corn',\n",
       "  'much',\n",
       "  'intact',\n",
       "  'kernals',\n",
       "  'hours',\n",
       "  'woke',\n",
       "  'fine',\n",
       "  'tempted',\n",
       "  'sign',\n",
       "  'acute',\n",
       "  'corn',\n",
       "  'intoxication',\n",
       "  'gordon',\n",
       "  'banks',\n",
       "  'n',\n",
       "  'jxp',\n",
       "  'skepticism',\n",
       "  'chastity',\n",
       "  'intellect',\n",
       "  'geb',\n",
       "  'cadre',\n",
       "  'dsl',\n",
       "  'pitt',\n",
       "  'edu',\n",
       "  'shameful',\n",
       "  'surrender',\n",
       "  'soon'],\n",
       " ['geb',\n",
       "  'cs',\n",
       "  'pitt',\n",
       "  'edu',\n",
       "  'gordon',\n",
       "  'banks',\n",
       "  'subject',\n",
       "  'helium',\n",
       "  'non',\n",
       "  'renewable',\n",
       "  'many',\n",
       "  'mris',\n",
       "  'reply',\n",
       "  'geb',\n",
       "  'cs',\n",
       "  'pitt',\n",
       "  'edu',\n",
       "  'gordon',\n",
       "  'banks',\n",
       "  'organization',\n",
       "  'univ',\n",
       "  'pittsburgh',\n",
       "  'computer',\n",
       "  'science',\n",
       "  'lines',\n",
       "  'article',\n",
       "  'lsj',\n",
       "  'gdinnkor',\n",
       "  'saltillo',\n",
       "  'cs',\n",
       "  'utexas',\n",
       "  'edu',\n",
       "  'turpin',\n",
       "  'cs',\n",
       "  'utexas',\n",
       "  'edu',\n",
       "  'russell',\n",
       "  'turpin',\n",
       "  'writes',\n",
       "  'helium',\n",
       "  'get',\n",
       "  'consumed',\n",
       "  'would',\n",
       "  'thought',\n",
       "  'failure',\n",
       "  'contain',\n",
       "  'perfectly',\n",
       "  'would',\n",
       "  'result',\n",
       "  'evaporation',\n",
       "  'back',\n",
       "  'atmosphere',\n",
       "  'sounds',\n",
       "  'like',\n",
       "  'cycle',\n",
       "  'obviously',\n",
       "  'takes',\n",
       "  'energy',\n",
       "  'run',\n",
       "  'cycle',\n",
       "  'seriously',\n",
       "  'doubt',\n",
       "  'helium',\n",
       "  'consumption',\n",
       "  'resource',\n",
       "  'issue',\n",
       "  'cycle',\n",
       "  'free',\n",
       "  'helium',\n",
       "  'escape',\n",
       "  'atmosphere',\n",
       "  'due',\n",
       "  'high',\n",
       "  'velocity',\n",
       "  'practical',\n",
       "  'recover',\n",
       "  'mined',\n",
       "  'gordon',\n",
       "  'banks',\n",
       "  'n',\n",
       "  'jxp',\n",
       "  'skepticism',\n",
       "  'chastity',\n",
       "  'intellect',\n",
       "  'geb',\n",
       "  'cadre',\n",
       "  'dsl',\n",
       "  'pitt',\n",
       "  'edu',\n",
       "  'shameful',\n",
       "  'surrender',\n",
       "  'soon']]"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t0IVVELfQ2uM",
    "outputId": "607e1d76-035a-4a5d-cba5-71bbf020a6b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.91 s, sys: 39.4 ms, total: 7.95 s\n",
      "Wall time: 4.94 s\n"
     ]
    }
   ],
   "source": [
    "%time model_data = word2vec.Word2Vec(corpus, workers=4, min_count=10, window=10, sample=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eb1e18caRSQF"
   },
   "outputs": [],
   "source": [
    "def sentiment(v, c):\n",
    "    model = Pipeline(\n",
    "        [(\"vectorizer\", v), \n",
    "         (\"classifier\", c)])\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print_accuracy_score_for_classes(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ztWQEkkRWa3"
   },
   "outputs": [],
   "source": [
    "class EmbeddingVectorizer(object):\n",
    "    '''\n",
    "    Для текста усредним вектора входящих в него слов\n",
    "    '''\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.size = model.vector_size\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([np.mean(\n",
    "            [self.model[w] for w in words if w in self.model] \n",
    "            or [np.zeros(self.size)], axis=0)\n",
    "            for words in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yw_0AdNGRZWr"
   },
   "outputs": [],
   "source": [
    "# Обучающая и тестовая выборки\n",
    "boundary = 700\n",
    "X_train = corpus[:boundary] \n",
    "X_test = corpus[boundary:]\n",
    "y_train = newsgroups['target'][:boundary]\n",
    "y_test = newsgroups['target'][boundary:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2WtAlPq5RfV3",
    "outputId": "274b5873-de2a-4e6f-9147-b361972ba105"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метка \t Accuracy\n",
      "0 \t 0.9003021148036254\n",
      "1 \t 0.8833746898263027\n",
      "2 \t 0.837772397094431\n",
      "3 \t 0.8365122615803815\n"
     ]
    }
   ],
   "source": [
    "sentiment(EmbeddingVectorizer(model_data.wv), LogisticRegression(C=5.0))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "lr6",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
